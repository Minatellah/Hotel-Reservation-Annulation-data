# -*- coding: utf-8 -*-
"""nehal minatellah sourour G02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_4U73OCTl3-OFX1-Yw4NJoaQLzXPJqwx

in this model i tried so many preprocessing thechniques and these ones resulted in better rate of accuracy
When i droped the 'reservation_status_date' variable without extracting its components, the model overfitted, so i had to treat this variable alone and extract informations from it
also 'agent' and 'company' variables have so many missing values but we dont have to drop them because it seems that the model is learning from them
also i used the log transformation because it resluts in better accuracy (not the standard or the min max)
for the outliers i handled them at first and also the log transformation helps to reduce their impact
for categorical variables i didnt use the one hot encoding because it will increase the number of variables so i encoded them
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
df = pd.read_csv('/content/drive/MyDrive/test ANN/hotel_bookings[1].csv')
df

import matplotlib.pyplot as plt
import seaborn as sns
import datetime
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import seaborn as sns
import tensorflow
from tensorflow import keras
from keras import layers
from keras.models import Sequential
from keras.optimizers import Adam
from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score

df.info()

df['is_canceled'].value_counts()

"""imbalenced data"""

for column in df.columns:
    fig = px.box(df, y=column, title=f'Box Plot for {column}')

    # Update layout to center the title and make it bold
    fig.update_layout(
        title=dict(text=f'<b>Box Plot for {column}</b>', x=0.5),
        boxmode='group'
    )

    fig.show()

cols_outliers = ['lead_time', 'stays_in_weekend_nights', 'adults', 'babies', 'previous_cancellations', 'booking_changes',
                 'days_in_waiting_list', ]


# Perform outlier handling for each specified column
for col_name in cols_outliers:
    # Calculate quartiles and IQR
    q1 = df[col_name].quantile(0.25)
    q3 = df[col_name].quantile(0.75)
    iqr = q3 - q1

    # Define the lower and upper bounds for outliers
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Replace outliers with the mean value of the column
    df[col_name] = np.where((df[col_name] > upper_bound) | (df[col_name] < lower_bound), np.mean(df[col_name]), df[col_name])

fig = px.histogram(df, x='is_canceled', color='is_canceled', title='Count Plot for Status of the cancelation')

fig.update_layout(
    title=dict(text='<b>Count Plot for Status of cancelations</b>', x=0.5)
)

fig.show()

df.isnull().sum()

"""here we can see that the variables agent and company have a lot of missing values so its better to remove them from the data because they are not going to be useful
we see also that children only has 4 missing values so we impute them with the 0
country also has 488 missing value so we impute them with 0

"""

df.fillna(0, inplace=True)

"""adults children and babies cant be 0 at the same time (it means that there is no guest)

"""

filter = (df.children == 0) & (df.adults == 0) & (df.babies == 0)
df[filter]

"""there is 188 row that has no meaning (0 adults kids and children at the same time)"""

df = df[~filter]
df

guest_country = df[df['is_canceled'] == 0] ['country'].value_counts()
guest_country_cols = ['country' , 'guests']
guest_country

"""we can see that these hotels have guests from all the world (166 country from 193 country), most of them are from PORTUGAL  and and other Europe countries


"""

nominal_var = list(set(df.select_dtypes(include=['object']).columns))
print(nominal_var)
numeric_var = list(set(df.select_dtypes(exclude=['object']).columns))
print(numeric_var)

df[nominal_var].describe().transpose()

"""we can see that most of the bookings are from Portugal , also August is the month where most of the guests visit the hotels (city hotels are the most visited) and this is due to summer vacation , most reserved room type is A"""

plt.figure(figsize = (24, 12))

numeric_df = df.select_dtypes(include=[int, float])
corr_matrix = numeric_df.corr()
sns.heatmap(corr_matrix, annot = True, linewidths = 1)
plt.show()

"""we can see that there is a lot if useless variables so we are going to drop them"""

useless_vars = ['days_in_waiting_list', 'arrival_date_year', 'assigned_room_type', 'booking_changes',
               'reservation_status', 'country', 'days_in_waiting_list']
df.drop(useless_vars , inplace=True , axis=1)

"""these variables are not highly correlated with the target variable"""

df.head()

#get the nominal variables
categorical_cols = [col for col in df.columns if df[col].dtype == 'O']
categorical_cols

cat_df = df[categorical_cols]
cat_df.head()

#extracting meaningful features from date information
cat_df['reservation_status_date'] = pd.to_datetime(cat_df['reservation_status_date'])

cat_df['year'] = cat_df['reservation_status_date'].dt.year
cat_df['month'] = cat_df['reservation_status_date'].dt.month
cat_df['day'] = cat_df['reservation_status_date'].dt.day

"""**this warning is because i modified a copy and not the original data frame**"""

#this variable was treated alone so we are going to drop it to treat the rest
cat_df.drop(['reservation_status_date','arrival_date_month'] , axis = 1, inplace = True)

cat_df.head()

#check fot unique values in each categorical variable
for col in cat_df.columns:
    print(f"{col}: \n{cat_df[col].unique()}\n")

cat_df['hotel'] = cat_df['hotel'].map({'Resort Hotel' : 0, 'City Hotel' : 1})

cat_df['year'] = cat_df['year'].map({2015: 0, 2014: 1, 2016: 2, 2017: 3}) #i encoded the year because it has huge values (2015,2014 ..... etc)

cat_df['meal'] = cat_df['meal'].map({'BB' : 0, 'FB': 1, 'HB': 2, 'SC': 3, 'Undefined': 4})

cat_df['market_segment'] =cat_df['market_segment'].map({'Direct': 0, 'Corporate': 1, 'Online TA': 2, 'Offline TA/TO': 3,
                                                           'Complementary': 4, 'Groups': 5, 'Undefined': 6, 'Aviation': 7})

cat_df['distribution_channel'] =cat_df['distribution_channel'].map({'Direct': 0, 'Corporate': 1, 'TA/TO': 2, 'Undefined': 3,
                                                                       'GDS': 4})

cat_df['reserved_room_type'] =cat_df['reserved_room_type'].map({'C': 0, 'A': 1, 'D': 2, 'E': 3, 'G': 4, 'F': 5, 'H': 6,
                                                                   'L': 7, 'B': 8})

cat_df['deposit_type'] = cat_df['deposit_type'].map({'No Deposit': 0, 'Refundable': 1, 'Non Refund': 3})

cat_df['customer_type'] = cat_df['customer_type'].map({'Transient': 0, 'Contract': 1, 'Transient-Party': 2, 'Group': 3})

cat_df.head()

#check for numeric variables
num_df = df.drop(columns = categorical_cols, axis = 1)
num_df.drop('is_canceled', axis = 1, inplace = True)
num_df

num_df.var()

#i used the log transformation because it leads to better results
num_df['lead_time'] = np.log(num_df['lead_time'] + 1)
num_df['arrival_date_week_number'] = np.log(num_df['arrival_date_week_number'] + 1)
num_df['arrival_date_day_of_month'] = np.log(num_df['arrival_date_day_of_month'] + 1)
num_df['agent'] = np.log(num_df['agent'] + 1)
num_df['company'] = np.log(num_df['company'] + 1)
num_df['adr'] = np.log(num_df['adr'] + 1)

"""**there is some invalid values that log cant handle them (zeros) thats why i added one to all the variables**"""

num_df.var()

num_df.isnull().sum()

#impute the missing value with the mean
num_df['adr'] = num_df['adr'].fillna(value = num_df['adr'].mean())

num_df.head()

x = pd.concat([cat_df, num_df], axis = 1) #because i preprocessed the categorical variables and numeric ones alone
y = df['is_canceled']  #the target variable

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20)

x_train.shape, x_test.shape, y_train.shape, y_test.shape

import keras
from keras.layers import Dense , Dropout , BatchNormalization
from keras.models import Sequential

model  = Sequential()
model.add(Dense(100, activation = 'relu', input_shape = [26]))
model.add(Dropout(rate=0.3)) #making it harder for the model to fit spurious patterns / prevent overfitting
model.add(Dense(100, activation = 'relu'))
model.add(Dropout(rate=0.3))
model.add(Dense(1, activation = 'sigmoid'))
model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
stop = keras.callbacks.EarlyStopping(patience=10, min_delta=0.001, restore_best_weights=True) #to avoid overfitting
history = model.fit(x_train, y_train, validation_split= 0.2, epochs = 1000, callbacks=[stop])

import plotly.express as px

plt.figure(figsize = (12, 6))

train_loss = history.history['loss']
val_loss = history.history['val_loss']
epoch = range(1, 59)

loss = pd.DataFrame({'train_loss' : train_loss, 'val_loss' : val_loss})

px.line(data_frame = loss, x = epoch, y = ['val_loss', 'train_loss'], title = 'Training and Validation Loss')

"""The early stopping callback did stop the training once the model started overfitting. Also, by including restore_best_weights we still get to keep the model where validation loss was lowest.
---


"""

plt.figure(figsize = (12, 6))

train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
epoch = range(1, 59) #because the model stopped at epoch 58


accuracy = pd.DataFrame({'train_acc' : train_acc, 'val_acc' : val_acc})

px.line(data_frame = accuracy, x = epoch, y = ['val_acc', 'train_acc'], title = 'Training and Validation Accuracy')

"""** the accuracy increase at the same rate as the cross-entropy fell**



"""

y_pred=model.predict(x_test)
y_pred=(y_pred>0.5)

from sklearn.metrics import confusion_matrix
Accurcy = accuracy_score(y_test, y_pred)
print("Accurcy=",Accurcy )

from sklearn.metrics import classification_report, confusion_matrix
conf_matrix=confusion_matrix(y_test,y_pred)
conf_matrix

sns.heatmap(conf_matrix,annot=True, fmt='d')
plt.show()

"""we can see that the model pedicted well the TP and TN (14968,8593)
the false predictions :
for the clients that did not cancel and the model predicted them that they canceled its 263
for the clients that canceled and the model predicted them that they didnt its 18 which is a low rate (the false negative rate is low)
"""
